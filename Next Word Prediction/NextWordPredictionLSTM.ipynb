{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNjiAedVukrGsqKVcf4hgb/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Ck95wL8qp-gb","colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":74},"executionInfo":{"status":"ok","timestamp":1662389317247,"user_tz":-330,"elapsed":20137,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}},"outputId":"50850643-5887-49e0-bb5a-33d7fd83fa9a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-9a4b4bae-69e4-48c8-88c3-c73c13b41241\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-9a4b4bae-69e4-48c8-88c3-c73c13b41241\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving Frankenstein.txt to Frankenstein.txt\n"]}],"source":["from google.colab import files\n","file = files.upload()"]},{"cell_type":"code","source":["f = open('Frankenstein.txt', 'r')\n","\n","lines = []\n","for i in f:\n","  lines.append(i)\n","text = ' '.join(lines)\n","text = text.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“', '').replace('”','').replace(\"'\", '').replace('.','').replace(',','').replace(':','').replace(';','')\n","text = text.split()\n","text = ' '.join(text)"],"metadata":{"id":"mFg3ipPurKNJ","executionInfo":{"status":"ok","timestamp":1662389323024,"user_tz":-330,"elapsed":451,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.text import Tokenizer\n","import pickle\n","import numpy as np"],"metadata":{"id":"xEgRX-ObJa92","executionInfo":{"status":"ok","timestamp":1662389326242,"user_tz":-330,"elapsed":2692,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts([text])\n","pickle.dump(tokenizer,open('tokenizer.pkl', 'wb'))\n","\n","sequences = tokenizer.texts_to_sequences([text])[0]"],"metadata":{"id":"3m6HlqtbJ7-b","executionInfo":{"status":"ok","timestamp":1662389326245,"user_tz":-330,"elapsed":15,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Tri-gram based model"],"metadata":{"id":"x1M3NX--Njhi"}},{"cell_type":"code","source":["X = []\n","y = []\n","\n","k = 3\n","n = len(sequences)\n","for i in range(0, n - k):\n","  X.append(sequences[i: i + k])\n","  y.append(sequences[i + k])\n","\n","X = np.array(X)\n","y = np.array(y)"],"metadata":{"id":"fBBN0I7xMQpR","executionInfo":{"status":"ok","timestamp":1662389327183,"user_tz":-330,"elapsed":571,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["from tensorflow.keras.utils import to_categorical"],"metadata":{"id":"VeMScnUbNry1","executionInfo":{"status":"ok","timestamp":1662389361738,"user_tz":-330,"elapsed":377,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["vocab_size = max(y) + 1\n","y = to_categorical(y, num_classes = vocab_size)"],"metadata":{"id":"8PGtz59TPCrl","executionInfo":{"status":"ok","timestamp":1662389385370,"user_tz":-330,"elapsed":1086,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Dense, Embedding, LSTM\n","from tensorflow.keras.optimizers import Adam"],"metadata":{"id":"d3xJ3bdBPTWW","executionInfo":{"status":"ok","timestamp":1662389450391,"user_tz":-330,"elapsed":382,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","lstm_out = 200\n","batch_size = 64\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, embed_dim, input_length = 3))\n","model.add(LSTM(lstm_out, return_sequences=True))\n","model.add(LSTM(lstm_out))\n","model.add(Dense(1000, activation ='relu'))\n","model.add(Dense(vocab_size, activation = 'softmax'))"],"metadata":{"id":"Qf5MsaHiPyz_","executionInfo":{"status":"ok","timestamp":1662390116498,"user_tz":-330,"elapsed":858,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C3-Z3tTZJqNM","executionInfo":{"status":"ok","timestamp":1662390117455,"user_tz":-330,"elapsed":19,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}},"outputId":"ac48fb38-af66-484d-cbcf-f7cd7acafed5"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, 3, 256)            1928704   \n","                                                                 \n"," lstm_2 (LSTM)               (None, 3, 200)            365600    \n","                                                                 \n"," lstm_3 (LSTM)               (None, 200)               320800    \n","                                                                 \n"," dense_2 (Dense)             (None, 1000)              201000    \n","                                                                 \n"," dense_3 (Dense)             (None, 7534)              7541534   \n","                                                                 \n","=================================================================\n","Total params: 10,357,638\n","Trainable params: 10,357,638\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["from keras.callbacks import ModelCheckpoint\n","checkpoint = ModelCheckpoint('next_words.h5', monitor = 'loss', verbose = 1, save_best_only = True)"],"metadata":{"id":"YqS2FOSaJ0lG","executionInfo":{"status":"ok","timestamp":1662390121538,"user_tz":-330,"elapsed":398,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["model.compile(loss='categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","model.fit(X, y, epochs = 100, batch_size = batch_size, callbacks=[checkpoint])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8NLsbrcwJ9bX","executionInfo":{"status":"ok","timestamp":1662391328975,"user_tz":-330,"elapsed":1205886,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}},"outputId":"a910988f-99d6-48b2-c8f5-cfe99bda2f8a"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 6.5121 - accuracy: 0.0701\n","Epoch 1: loss improved from inf to 6.51158, saving model to next_words.h5\n","1224/1224 [==============================] - 15s 10ms/step - loss: 6.5116 - accuracy: 0.0701\n","Epoch 2/100\n","1224/1224 [==============================] - ETA: 0s - loss: 5.8673 - accuracy: 0.1208\n","Epoch 2: loss improved from 6.51158 to 5.86729, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 5.8673 - accuracy: 0.1208\n","Epoch 3/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 5.4315 - accuracy: 0.1476\n","Epoch 3: loss improved from 5.86729 to 5.43142, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 5.4314 - accuracy: 0.1476\n","Epoch 4/100\n","1224/1224 [==============================] - ETA: 0s - loss: 5.0853 - accuracy: 0.1667\n","Epoch 4: loss improved from 5.43142 to 5.08529, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 5.0853 - accuracy: 0.1667\n","Epoch 5/100\n","1224/1224 [==============================] - ETA: 0s - loss: 4.7709 - accuracy: 0.1858\n","Epoch 5: loss improved from 5.08529 to 4.77092, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 4.7709 - accuracy: 0.1858\n","Epoch 6/100\n","1219/1224 [============================>.] - ETA: 0s - loss: 4.4523 - accuracy: 0.2069\n","Epoch 6: loss improved from 4.77092 to 4.45257, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 4.4526 - accuracy: 0.2069\n","Epoch 7/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 4.1041 - accuracy: 0.2308\n","Epoch 7: loss improved from 4.45257 to 4.10467, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 4.1047 - accuracy: 0.2306\n","Epoch 8/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 3.7323 - accuracy: 0.2605\n","Epoch 8: loss improved from 4.10467 to 3.73235, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 3.7323 - accuracy: 0.2605\n","Epoch 9/100\n","1224/1224 [==============================] - ETA: 0s - loss: 3.3311 - accuracy: 0.3032\n","Epoch 9: loss improved from 3.73235 to 3.33112, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 3.3311 - accuracy: 0.3032\n","Epoch 10/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 2.9188 - accuracy: 0.3569\n","Epoch 10: loss improved from 3.33112 to 2.91892, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 2.9189 - accuracy: 0.3569\n","Epoch 11/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 2.5310 - accuracy: 0.4228\n","Epoch 11: loss improved from 2.91892 to 2.53103, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 2.5310 - accuracy: 0.4228\n","Epoch 12/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 2.1939 - accuracy: 0.4826\n","Epoch 12: loss improved from 2.53103 to 2.19420, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 2.1942 - accuracy: 0.4826\n","Epoch 13/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 1.9084 - accuracy: 0.5366\n","Epoch 13: loss improved from 2.19420 to 1.90859, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 1.9086 - accuracy: 0.5365\n","Epoch 14/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 1.6674 - accuracy: 0.5839\n","Epoch 14: loss improved from 1.90859 to 1.66820, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 1.6682 - accuracy: 0.5837\n","Epoch 15/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 1.4558 - accuracy: 0.6286\n","Epoch 15: loss improved from 1.66820 to 1.45618, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 1.4562 - accuracy: 0.6285\n","Epoch 16/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 1.2857 - accuracy: 0.6660\n","Epoch 16: loss improved from 1.45618 to 1.28625, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 1.2862 - accuracy: 0.6660\n","Epoch 17/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 1.1351 - accuracy: 0.7006\n","Epoch 17: loss improved from 1.28625 to 1.13556, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 1.1356 - accuracy: 0.7006\n","Epoch 18/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 1.0057 - accuracy: 0.7294\n","Epoch 18: loss improved from 1.13556 to 1.00574, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 1.0057 - accuracy: 0.7294\n","Epoch 19/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.9024 - accuracy: 0.7553\n","Epoch 19: loss improved from 1.00574 to 0.90243, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.9024 - accuracy: 0.7553\n","Epoch 20/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.8175 - accuracy: 0.7769\n","Epoch 20: loss improved from 0.90243 to 0.81788, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.8179 - accuracy: 0.7769\n","Epoch 21/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.7444 - accuracy: 0.7943\n","Epoch 21: loss improved from 0.81788 to 0.74470, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.7447 - accuracy: 0.7943\n","Epoch 22/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.6839 - accuracy: 0.8107\n","Epoch 22: loss improved from 0.74470 to 0.68439, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.6844 - accuracy: 0.8105\n","Epoch 23/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.6299 - accuracy: 0.8238\n","Epoch 23: loss improved from 0.68439 to 0.63049, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.6305 - accuracy: 0.8236\n","Epoch 24/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.5962 - accuracy: 0.8320\n","Epoch 24: loss improved from 0.63049 to 0.59647, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.5965 - accuracy: 0.8319\n","Epoch 25/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.5605 - accuracy: 0.8400\n","Epoch 25: loss improved from 0.59647 to 0.56049, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.5605 - accuracy: 0.8400\n","Epoch 26/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.5270 - accuracy: 0.8487\n","Epoch 26: loss improved from 0.56049 to 0.52703, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.5270 - accuracy: 0.8487\n","Epoch 27/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.5055 - accuracy: 0.8528\n","Epoch 27: loss improved from 0.52703 to 0.50555, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.5055 - accuracy: 0.8528\n","Epoch 28/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.4898 - accuracy: 0.8560\n","Epoch 28: loss improved from 0.50555 to 0.49025, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.4903 - accuracy: 0.8559\n","Epoch 29/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.4725 - accuracy: 0.8588\n","Epoch 29: loss improved from 0.49025 to 0.47283, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.4728 - accuracy: 0.8588\n","Epoch 30/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.4520 - accuracy: 0.8647\n","Epoch 30: loss improved from 0.47283 to 0.45271, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.4527 - accuracy: 0.8645\n","Epoch 31/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.4432 - accuracy: 0.8647\n","Epoch 31: loss improved from 0.45271 to 0.44332, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.4433 - accuracy: 0.8647\n","Epoch 32/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.4405 - accuracy: 0.8640\n","Epoch 32: loss improved from 0.44332 to 0.44065, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.4406 - accuracy: 0.8639\n","Epoch 33/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.4184 - accuracy: 0.8680\n","Epoch 33: loss improved from 0.44065 to 0.41849, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.4185 - accuracy: 0.8679\n","Epoch 34/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.4093 - accuracy: 0.8702\n","Epoch 34: loss improved from 0.41849 to 0.40920, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.4092 - accuracy: 0.8702\n","Epoch 35/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3980 - accuracy: 0.8719\n","Epoch 35: loss improved from 0.40920 to 0.39831, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3983 - accuracy: 0.8718\n","Epoch 36/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.3992 - accuracy: 0.8691\n","Epoch 36: loss did not improve from 0.39831\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3993 - accuracy: 0.8691\n","Epoch 37/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.3892 - accuracy: 0.8714\n","Epoch 37: loss improved from 0.39831 to 0.38911, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3891 - accuracy: 0.8715\n","Epoch 38/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3817 - accuracy: 0.8727\n","Epoch 38: loss improved from 0.38911 to 0.38174, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3817 - accuracy: 0.8726\n","Epoch 39/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.3748 - accuracy: 0.8738\n","Epoch 39: loss improved from 0.38174 to 0.37483, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3748 - accuracy: 0.8738\n","Epoch 40/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.3729 - accuracy: 0.8726\n","Epoch 40: loss improved from 0.37483 to 0.37298, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3730 - accuracy: 0.8725\n","Epoch 41/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3664 - accuracy: 0.8734\n","Epoch 41: loss improved from 0.37298 to 0.36653, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3665 - accuracy: 0.8733\n","Epoch 42/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3598 - accuracy: 0.8740\n","Epoch 42: loss improved from 0.36653 to 0.35997, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3600 - accuracy: 0.8740\n","Epoch 43/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.3572 - accuracy: 0.8743\n","Epoch 43: loss improved from 0.35997 to 0.35763, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3576 - accuracy: 0.8742\n","Epoch 44/100\n","1219/1224 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8757\n","Epoch 44: loss improved from 0.35763 to 0.34845, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3485 - accuracy: 0.8756\n","Epoch 45/100\n","1219/1224 [============================>.] - ETA: 0s - loss: 0.3482 - accuracy: 0.8752\n","Epoch 45: loss did not improve from 0.34845\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3485 - accuracy: 0.8752\n","Epoch 46/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.3448 - accuracy: 0.8746\n","Epoch 46: loss improved from 0.34845 to 0.34522, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3452 - accuracy: 0.8745\n","Epoch 47/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.3443 - accuracy: 0.8747\n","Epoch 47: loss improved from 0.34522 to 0.34433, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3443 - accuracy: 0.8747\n","Epoch 48/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.3369 - accuracy: 0.8759\n","Epoch 48: loss improved from 0.34433 to 0.33685, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3369 - accuracy: 0.8759\n","Epoch 49/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.3346 - accuracy: 0.8766\n","Epoch 49: loss improved from 0.33685 to 0.33476, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3348 - accuracy: 0.8766\n","Epoch 50/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3349 - accuracy: 0.8761\n","Epoch 50: loss did not improve from 0.33476\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3350 - accuracy: 0.8760\n","Epoch 51/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.3256 - accuracy: 0.8768\n","Epoch 51: loss improved from 0.33476 to 0.32570, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3257 - accuracy: 0.8768\n","Epoch 52/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.3288 - accuracy: 0.8772\n","Epoch 52: loss did not improve from 0.32570\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3288 - accuracy: 0.8772\n","Epoch 53/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3210 - accuracy: 0.8781\n","Epoch 53: loss improved from 0.32570 to 0.32107, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3211 - accuracy: 0.8780\n","Epoch 54/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.3216 - accuracy: 0.8773\n","Epoch 54: loss did not improve from 0.32107\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3218 - accuracy: 0.8773\n","Epoch 55/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.3250 - accuracy: 0.8749\n","Epoch 55: loss did not improve from 0.32107\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3250 - accuracy: 0.8749\n","Epoch 56/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.3165 - accuracy: 0.8773\n","Epoch 56: loss improved from 0.32107 to 0.31651, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3165 - accuracy: 0.8773\n","Epoch 57/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.3119 - accuracy: 0.8779\n","Epoch 57: loss improved from 0.31651 to 0.31193, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3119 - accuracy: 0.8779\n","Epoch 58/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.3120 - accuracy: 0.8777\n","Epoch 58: loss did not improve from 0.31193\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3120 - accuracy: 0.8777\n","Epoch 59/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.3118 - accuracy: 0.8777\n","Epoch 59: loss improved from 0.31193 to 0.31182, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3118 - accuracy: 0.8777\n","Epoch 60/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.8777\n","Epoch 60: loss improved from 0.31182 to 0.30893, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3089 - accuracy: 0.8778\n","Epoch 61/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.3122 - accuracy: 0.8775\n","Epoch 61: loss did not improve from 0.30893\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3123 - accuracy: 0.8774\n","Epoch 62/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.3077 - accuracy: 0.8760\n","Epoch 62: loss improved from 0.30893 to 0.30794, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3079 - accuracy: 0.8759\n","Epoch 63/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.3018 - accuracy: 0.8782\n","Epoch 63: loss improved from 0.30794 to 0.30185, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3018 - accuracy: 0.8782\n","Epoch 64/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3006 - accuracy: 0.8783\n","Epoch 64: loss improved from 0.30185 to 0.30076, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3008 - accuracy: 0.8782\n","Epoch 65/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.3043 - accuracy: 0.8769\n","Epoch 65: loss did not improve from 0.30076\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3047 - accuracy: 0.8768\n","Epoch 66/100\n","1219/1224 [============================>.] - ETA: 0s - loss: 0.3034 - accuracy: 0.8771\n","Epoch 66: loss did not improve from 0.30076\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3036 - accuracy: 0.8769\n","Epoch 67/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.3039 - accuracy: 0.8762\n","Epoch 67: loss did not improve from 0.30076\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.3040 - accuracy: 0.8762\n","Epoch 68/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2979 - accuracy: 0.8778\n","Epoch 68: loss improved from 0.30076 to 0.29811, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2981 - accuracy: 0.8777\n","Epoch 69/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.2950 - accuracy: 0.8789\n","Epoch 69: loss improved from 0.29811 to 0.29510, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2951 - accuracy: 0.8789\n","Epoch 70/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2995 - accuracy: 0.8775\n","Epoch 70: loss did not improve from 0.29510\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2997 - accuracy: 0.8774\n","Epoch 71/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2988 - accuracy: 0.8770\n","Epoch 71: loss did not improve from 0.29510\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2990 - accuracy: 0.8769\n","Epoch 72/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.2859 - accuracy: 0.8804\n","Epoch 72: loss improved from 0.29510 to 0.28587, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2859 - accuracy: 0.8804\n","Epoch 73/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.2870 - accuracy: 0.8799\n","Epoch 73: loss did not improve from 0.28587\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2870 - accuracy: 0.8799\n","Epoch 74/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.2913 - accuracy: 0.8781\n","Epoch 74: loss did not improve from 0.28587\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2914 - accuracy: 0.8780\n","Epoch 75/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.2945 - accuracy: 0.8782\n","Epoch 75: loss did not improve from 0.28587\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2944 - accuracy: 0.8782\n","Epoch 76/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.2894 - accuracy: 0.8785\n","Epoch 76: loss did not improve from 0.28587\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2894 - accuracy: 0.8786\n","Epoch 77/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2928 - accuracy: 0.8780\n","Epoch 77: loss did not improve from 0.28587\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2928 - accuracy: 0.8780\n","Epoch 78/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.2875 - accuracy: 0.8783\n","Epoch 78: loss did not improve from 0.28587\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2875 - accuracy: 0.8783\n","Epoch 79/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2820 - accuracy: 0.8790\n","Epoch 79: loss improved from 0.28587 to 0.28198, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2820 - accuracy: 0.8790\n","Epoch 80/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.2818 - accuracy: 0.8803\n","Epoch 80: loss improved from 0.28198 to 0.28195, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2820 - accuracy: 0.8802\n","Epoch 81/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2904 - accuracy: 0.8772\n","Epoch 81: loss did not improve from 0.28195\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2906 - accuracy: 0.8771\n","Epoch 82/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2901 - accuracy: 0.8764\n","Epoch 82: loss did not improve from 0.28195\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2906 - accuracy: 0.8762\n","Epoch 83/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2776 - accuracy: 0.8798\n","Epoch 83: loss improved from 0.28195 to 0.27759, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2776 - accuracy: 0.8798\n","Epoch 84/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.2742 - accuracy: 0.8812\n","Epoch 84: loss improved from 0.27759 to 0.27427, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2743 - accuracy: 0.8812\n","Epoch 85/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2824 - accuracy: 0.8796\n","Epoch 85: loss did not improve from 0.27427\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2827 - accuracy: 0.8795\n","Epoch 86/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2918 - accuracy: 0.8766\n","Epoch 86: loss did not improve from 0.27427\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2920 - accuracy: 0.8765\n","Epoch 87/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2867 - accuracy: 0.8784\n","Epoch 87: loss did not improve from 0.27427\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2870 - accuracy: 0.8783\n","Epoch 88/100\n","1219/1224 [============================>.] - ETA: 0s - loss: 0.2790 - accuracy: 0.8790\n","Epoch 88: loss did not improve from 0.27427\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2790 - accuracy: 0.8790\n","Epoch 89/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2763 - accuracy: 0.8797\n","Epoch 89: loss did not improve from 0.27427\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2763 - accuracy: 0.8797\n","Epoch 90/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.2738 - accuracy: 0.8800\n","Epoch 90: loss improved from 0.27427 to 0.27371, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2737 - accuracy: 0.8800\n","Epoch 91/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2810 - accuracy: 0.8793\n","Epoch 91: loss did not improve from 0.27371\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2810 - accuracy: 0.8793\n","Epoch 92/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.2856 - accuracy: 0.8771\n","Epoch 92: loss did not improve from 0.27371\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2856 - accuracy: 0.8771\n","Epoch 93/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.2779 - accuracy: 0.8790\n","Epoch 93: loss did not improve from 0.27371\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2780 - accuracy: 0.8789\n","Epoch 94/100\n","1220/1224 [============================>.] - ETA: 0s - loss: 0.2684 - accuracy: 0.8813\n","Epoch 94: loss improved from 0.27371 to 0.26850, saving model to next_words.h5\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2685 - accuracy: 0.8813\n","Epoch 95/100\n","1222/1224 [============================>.] - ETA: 0s - loss: 0.2704 - accuracy: 0.8810\n","Epoch 95: loss did not improve from 0.26850\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2705 - accuracy: 0.8810\n","Epoch 96/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2770 - accuracy: 0.8794\n","Epoch 96: loss did not improve from 0.26850\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2771 - accuracy: 0.8794\n","Epoch 97/100\n","1219/1224 [============================>.] - ETA: 0s - loss: 0.2748 - accuracy: 0.8803\n","Epoch 97: loss did not improve from 0.26850\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2752 - accuracy: 0.8802\n","Epoch 98/100\n","1223/1224 [============================>.] - ETA: 0s - loss: 0.2803 - accuracy: 0.8777\n","Epoch 98: loss did not improve from 0.26850\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2803 - accuracy: 0.8777\n","Epoch 99/100\n","1224/1224 [==============================] - ETA: 0s - loss: 0.2767 - accuracy: 0.8792\n","Epoch 99: loss did not improve from 0.26850\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2767 - accuracy: 0.8792\n","Epoch 100/100\n","1221/1224 [============================>.] - ETA: 0s - loss: 0.2707 - accuracy: 0.8802\n","Epoch 100: loss did not improve from 0.26850\n","1224/1224 [==============================] - 12s 10ms/step - loss: 0.2709 - accuracy: 0.8801\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f32522de810>"]},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["from keras.models import load_model\n","model = load_model('next_words.h5')\n","tokenizer = pickle.load(open('tokenizer.pkl','rb'))"],"metadata":{"id":"_nUhBboHKTeM","executionInfo":{"status":"ok","timestamp":1662391958621,"user_tz":-330,"elapsed":1694,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["while True:\n","  text = input('Please enter sentence(Please write more than 2 words), 0 to exit: ')\n","  if text != '0':\n","    try:\n","      text = text.split(' ')\n","      text = text[-3: ]\n","      sequence = np.array(tokenizer.texts_to_sequences([text]))\n","      #print(model.predict(sequence)[0])\n","      output = np.argpartition(model.predict(sequence)[0], -5)[-5:]\n","\n","      next_words = []\n","      for key, value in tokenizer.word_index.items():\n","        if value in output:\n","          next_words.append(key)\n","        if len(next_words) == 5:\n","          break\n","      print(next_words)\n","    except Exception as e:\n","      print(e)\n","  else:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2VKLcoklPtP7","executionInfo":{"status":"ok","timestamp":1662392430714,"user_tz":-330,"elapsed":46662,"user":{"displayName":"Debojyoti Biswas","userId":"06823093624328995488"}},"outputId":"a85a6e2d-9d93-42b8-dc30-5949d45b69d4"},"execution_count":48,"outputs":[{"name":"stdout","output_type":"stream","text":["Please enter sentence(Please write more than 2 words), 0 to exit: I perused, for the first time\n","['the', 'i', 'saw', 'those', 'during']\n","Please enter sentence(Please write more than 2 words), 0 to exit: Six years have passed\n","['and', 'in', 'more', 'elizabeth', 'since']\n","Please enter sentence(Please write more than 2 words), 0 to exit: 0\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Ob_pvIFOQu5N"},"execution_count":null,"outputs":[]}]}